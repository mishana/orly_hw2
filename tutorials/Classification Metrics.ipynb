{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [Cross validation evaluation](#cv_evaluation)\n",
    "* [Confusion matrix](#confusion_matrix)\n",
    "* [Precision-Recall](#precision_recall)\n",
    "* [ROC curve](#roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# inline plot \n",
    "%matplotlib inline  \n",
    "# default figure size \n",
    "matplotlib.rcParams['figure.figsize'] = (20, 10)\n",
    "# to make our sets reproducible \n",
    "np.random.seed(42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "mnist.target = mnist.target.astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn.datasets as datasets\n",
    "# mnist = datasets.load_digits()\n",
    "\n",
    "X, y = mnist.data, mnist.target\n",
    "# split \n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "# reduce to binary classification for the digit 5\n",
    "y_train_5 = (y_train == 5)\n",
    "y_test_5 = (y_test == 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples, num_features = X_train.shape\n",
    "print(f'''\n",
    "Number of samples {num_samples}\n",
    "Number of features {num_features}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)\n",
    "sgd_clf.fit(X_train, y_train_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation evaluation <a class=\"anchor\" id=\"cv_evaluation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cv_accuracy = cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n",
    "print(f'''cross validations accuracies: {cv_accuracy}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion_matrix <a class=\"anchor\" id=\"confusion_matrix\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is split according to the cv parameter. \n",
    "# Each sample belongs to exactly one test set, and its prediction is computed \n",
    "# with an estimator fitted on the corresponding training set.\n",
    "# Try not to use - only for the sake of example!!!\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_predictions = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method=\"predict\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_train_5, y_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
    "y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
    "confusion_matrix(y_true, y_pred, labels=[\"ant\", \"bird\", \"cat\", \"tal\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilabel confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "# class 1 is the first column predictions and labels (y_true) and class 2 is the second\n",
    "y_true = np.array([[0,0], [0,1], [1,1], [0,1], [0,1], [1,1]])\n",
    "y_pred = np.array([[1,1], [0,1], [0,1], [1,0], [0,1], [1,1] ])\n",
    "# generates confusion matrix per label\n",
    "m = multilabel_confusion_matrix(y_true, y_pred)\n",
    "print(f\"\"\"\n",
    "confusion matrix for class 1: \n",
    "{m[0]}\n",
    "confusion matrix for class 2: \n",
    "{m[1]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision - Recall <a class=\"anchor\" id=\"precision_recall\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "y_true = [0, 1, 2, 0, 1, 2, 2]\n",
    "y_pred = [1, 2, 1, 0, 1, 1, 2]\n",
    "\n",
    "# 'micro':\n",
    "# Calculate metrics globally by counting the total true positives,\n",
    "# false negatives and false positives.\n",
    "# 'macro':\n",
    "# Calculate metrics for each label, and find their unweighted\n",
    "# mean.  This does not take label imbalance into account.\n",
    "precision_score(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicit way to calculate 'macro'\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "recall = np.diag(cm) / np.sum(cm, axis = 1)\n",
    "precision = np.diag(cm) / np.sum(cm, axis = 0)\n",
    "f1 = 2 * recall * precision / (recall + precision)\n",
    "print(f\"\"\"\n",
    "recall: {np.mean(recall)}\n",
    "precision: {np.mean(precision)}\n",
    "f1: {np.mean(f1)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "y_true_b = [0, 1, 1, 0, 1, 0, 0]\n",
    "y_pred_b = [0, 0, 1, 0, 0, 1, 0]\n",
    "print(f\"\"\"\n",
    "binary: {f1_score(y_true_b, y_pred_b, average='binary')}\n",
    "multiclass: {f1_score(y_true, y_pred, average='macro')}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### precision recall tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method=\"decision_function\")\n",
    "# thresholds = thresholds used to obtain each precision recall score \n",
    "precisions, recalls, thresholds = precision_recall_curve(y_true=y_train_5, probas_pred=y_scores )\n",
    "\n",
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "    plt.legend(loc=\"center right\", fontsize=16) # Not shown in the book\n",
    "    plt.xlabel(\"Threshold\", fontsize=16)        # Not shown\n",
    "    plt.grid(True)                              # Not shown\n",
    "    plt.axis([min(thresholds), max(thresholds), 0, 1])             # Not shown\n",
    "\n",
    "\n",
    "\n",
    "recall_90_precision = recalls[np.argmax(precisions >= 0.90)]\n",
    "print(f'recall for 0.9 precision: {recall_90_precision}')\n",
    "threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))                                                                  \n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "# plot the recall for the 0.9 precision\n",
    "plt.plot([threshold_90_precision, threshold_90_precision], [0., 0.9], \"r:\")                 \n",
    "plt.plot([min(thresholds), threshold_90_precision], [0.9, 0.9], \"r:\")                                \n",
    "plt.plot([min(thresholds), threshold_90_precision], [recall_90_precision, recall_90_precision], \"r:\")\n",
    "plt.plot([threshold_90_precision], [0.9], \"ro\")                                             \n",
    "plt.plot([threshold_90_precision], [recall_90_precision], \"ro\")                            \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_vs_recall(precisions, recalls):\n",
    "    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n",
    "    plt.xlabel(\"Recall\", fontsize=16)\n",
    "    plt.ylabel(\"Precision\", fontsize=16)\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_precision_vs_recall(precisions, recalls)\n",
    "# this is called recall@precision90\n",
    "plt.plot([recall_90_precision, recall_90_precision], [0., 0.9], \"r:\")\n",
    "plt.plot([0.0, recall_90_precision], [0.9, 0.9], \"r:\")\n",
    "plt.plot([recall_90_precision], [0.9], \"ro\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve <a class=\"anchor\" id=\"roc\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal\n",
    "    plt.axis([0, 1, 0, 1])                                    \n",
    "    plt.xlabel('False Positive Rate (Fall-Out)', fontsize=16) \n",
    "    plt.ylabel('True Positive Rate (Recall)', fontsize=16)    \n",
    "    plt.grid(True)                                            \n",
    "\n",
    "plt.figure(figsize=(8, 6))                                    \n",
    "plot_roc_curve(fpr, tpr)\n",
    "fpr_90 = fpr[np.argmax(tpr >= recall_90_precision)]           \n",
    "plt.plot([fpr_90, fpr_90], [0., recall_90_precision], \"r:\")   \n",
    "plt.plot([0.0, fpr_90], [recall_90_precision, recall_90_precision], \"r:\")  \n",
    "plt.plot([fpr_90], [recall_90_precision], \"ro\")               \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}