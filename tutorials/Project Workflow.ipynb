{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [Housing data](#Housingdata)\n",
    "* [Data Analysis](#analysis)\n",
    "* [Train-Test Split](#train_test_split)\n",
    "* [Feature Analysis and Extraction](#features_analysis)\n",
    "* [Missing Values](#missing_values)\n",
    "* [Categorical Features](#categorical_features)\n",
    "* [Feature Engineering](#feature_engineering)\n",
    "* [Train Models](#train_models)\n",
    "* [Cross Validation](#cross_validation)\n",
    "* [Hyper Prameter Optimization](#hyperparam_optimization)\n",
    "* [Final Model](#final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# inline plot \n",
    "%matplotlib inline  \n",
    "# default figure size \n",
    "matplotlib.rcParams['figure.figsize'] = (20, 10)\n",
    "# to make this notebook's output identical at every run\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Housing data <a class=\"anchor\" id=\"Housingdata\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "fetch_housing_data()\n",
    "housing = load_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # same dataset fewer, slightly different features, different naming \n",
    "# from sklearn.datasets import fetch_california_housing\n",
    "# housing = fetch_california_housing(as_frame=True)['frame']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis <a class=\"anchor\" id=\"analysis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# shuffle the dataset\n",
    "from sklearn.utils import shuffle\n",
    "housing = shuffle(housing)\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numbber of distinct values per field\n",
    "housing[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics of the fields \n",
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histograms\n",
    "import matplotlib.pyplot as plt\n",
    "_ = housing.hist(bins=50, figsize=(20,15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train-test split and stratified split <a class=\"anchor\" id=\"train_test_split\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "print(f'''\n",
    "Train set size: {train_set.size}\n",
    "Test set size: {test_set.size}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descretisizing the response (label)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(16,8))\n",
    "housing[\"median_income\"].hist(ax=axes[0])\n",
    "# Bin values into discrete intervals.\n",
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "housing[\"income_cat\"].hist(ax=axes[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# here we see the difference between a stratidfied split and \n",
    "# a random one. we see that the proportions of the labels are better preserved\n",
    "\n",
    "# StratifiedShuffleSplit\n",
    "# =======================\n",
    "# This cross-validation object is a merge of StratifiedKFold and\n",
    "# ShuffleSplit, which returns stratified randomized folds. The folds\n",
    "# are made by preserving the percentage of samples for each class.\n",
    "\n",
    "# split.split Generate indices to split data into training and test set.\n",
    "# note that each split will maintain the proportions of the classes in train and test\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "# just one loop here\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]\n",
    "\n",
    "# verifying the stratification\n",
    "def income_cat_proportions(data):\n",
    "    return data[\"income_cat\"].value_counts() / len(data)\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "\n",
    "compare_props = pd.DataFrame({\n",
    "    \"Overall\": income_cat_proportions(housing),\n",
    "    \"Stratified\": income_cat_proportions(strat_test_set),\n",
    "    \"Random\": income_cat_proportions(test_set),\n",
    "}).sort_index()\n",
    "compare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] / compare_props[\"Overall\"] - 100\n",
    "compare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] / compare_props[\"Overall\"] - 100\n",
    "compare_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove previously generated binned label\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Analysis and Extraction <a class=\"anchor\" id=\"features_analysis\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the locations \n",
    "# we use alpha=0.1 to visualize thhe density better\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locations with population and median house value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another more comprehensive visualization \n",
    "# \"s\" = the size ot each location is set by the size if the population\n",
    "# \"c\" = the color indicates the median_house_value \n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n",
    "             s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n",
    "             c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
    "             sharex=False)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## correlation matrix \n",
    "# min_periods: minimum number of observations required per pair of columns\n",
    "# to have a valid result\n",
    "corr_matrix = housing.corr(method='pearson', min_periods=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the last column we see the correlation between the different features and the median \n",
    "# house value. we see that it is highly correlated with median income. Note that the values \n",
    "# indicate linear relationship, when values are low it doesn't say that the fields are independet \n",
    "\n",
    "corr_matrix.sort_values(by='median_house_value', key=lambda x: abs(x), ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
    "              \"housing_median_age\"]\n",
    "\n",
    "# a scatter visualizing densities of the pairs of selected variables\n",
    "# among the ones specified. Note that on the diagonal we see histograms of the \n",
    "# values of the variables since if we would have plot a scatter, the plot was \n",
    "# the histogram drown on the diagonal, so no need for 3d representation\n",
    "\n",
    "# we see that median income is highly correlated with median housing value\n",
    "\n",
    "_ = scatter_matrix(housing[attributes], figsize=(12, 8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### correlations with labels (response variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the total_rooms, total_bedrooms are dependent on the households, total_rooms \n",
    "# we will try to define better features\n",
    "\n",
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]\n",
    "\n",
    "# we see that bedrooms_per_room is a great new feature (the minus sign tells us \n",
    "# the hier the ratio of bedrooms the lower the median house value \n",
    "# rooms_per_household looks nice \n",
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(key=lambda x: abs(x), ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split labels from features\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1) \n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values <a class=\"anchor\" id=\"missing_values\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete_rows = housing[housing.isnull().any(axis=1)]\n",
    "print(f'Number of incomplete rows: {incomplete_rows.size}')\n",
    "incomplete_rows.isnull().sum(axis=0).reset_index().rename(columns={'index': 'field', 0: 'missing'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_incomplete_rows = housing[housing.isnull().any(axis=1)].head()\n",
    "sample_incomplete_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1: drop the rows \n",
    "sample_incomplete_rows.dropna(subset=[\"total_bedrooms\"])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# option 2: remove the feature \n",
    "sample_incomplete_rows.drop(\"total_bedrooms\", axis=1)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 3: impute - should be very carefull with imputing data this way - it changes the \n",
    "# distribbution of values. this won't effect the model only under very specific circumstances \n",
    "median = housing[\"total_bedrooms\"].median()\n",
    "sample_incomplete_rows[\"total_bedrooms\"].fillna(median, inplace=True) \n",
    "sample_incomplete_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_numerical_features = housing.select_dtypes(include=[np.number])\n",
    "imputer.fit(housing_numerical_features)\n",
    "\n",
    "print(f'The median per field for imputation {imputer.statistics_}')\n",
    "\n",
    "# create a dataframe with the imputation - we want to retain the index \n",
    "# so we can access specific rows \n",
    "housing_tr = pd.DataFrame(imputer.transform(housing_numerical_features), \n",
    "                          columns=housing_numerical_features.columns,\n",
    "                          index=housing.index)\n",
    "\n",
    "housing_tr.loc[sample_incomplete_rows.index.values]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical features <a class=\"anchor\" id=\"categorical_features\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we encode the categorical feature as numbers\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing[[\"ocean_proximity\"]])\n",
    "print(f'categories: {ordinal_encoder.categories_}')\n",
    "pd.DataFrame(housing_cat_encoded[:10], housing[[\"ocean_proximity\"]][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onehot encoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cat_encoder = OneHotEncoder(sparse=False)\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing[[\"ocean_proximity\"]])\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering <a class=\"anchor\" id=\"feature_engineering\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# column index\n",
    "col_names = \"total_rooms\", \"total_bedrooms\", \"population\", \"households\"\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = [\n",
    "    housing.columns.get_loc(c) for c in col_names] # get the column indices\n",
    "\n",
    "######################################\n",
    "# define a new feature using sklearn #\n",
    "######################################\n",
    "class CombinedAttributesAdder(TransformerMixin):\n",
    "    \n",
    "    def __init__(self, add_bedrooms_per_room=True):\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # nothing to learn befor transforming\n",
    "        return self  \n",
    "    \n",
    "    def transform(self, X):\n",
    "        rooms_per_household = np.expand_dims(X[:, rooms_ix] / X[:, households_ix], axis=1)\n",
    "        population_per_household = np.expand_dims(X[:, population_ix] / X[:, households_ix], axis=1)\n",
    "        res = [X, rooms_per_household, population_per_household]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = np.expand_dims(X[:, bedrooms_ix] / X[:, rooms_ix],1)\n",
    "            res.append(bedrooms_per_room)\n",
    "            \n",
    "        return np.concatenate(res,axis=1)\n",
    "\n",
    "#################################\n",
    "# see that it works as expected #\n",
    "#################################\n",
    "\n",
    "# attr_extractor = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "\n",
    "# housing_extra_attribs = pd.DataFrame(\n",
    "#     attr_extractor.transform(housing.values),\n",
    "#     columns=list(housing.columns)+[\"rooms_per_household\", \"population_per_household\"],\n",
    "#     index=housing.index)\n",
    "# housing_extra_attribs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline the transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "numericals_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")), # impute missing values\n",
    "        ('attribs_adder', CombinedAttributesAdder()), # add new features\n",
    "        ('std_scaler', StandardScaler()), # scale the features\n",
    "    ])\n",
    "\n",
    "housing_num_tr = numericals_pipeline.fit_transform(housing_numerical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "numeric_attribs = list(housing_numerical_features)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "# ColumnTransformer: Applies transformers to columns of an array or pandas DataFrame.\n",
    "# This is useful for heterogeneous or columnar data, to combine several feature extraction \n",
    "# mechanisms or transformations into a single transformer.\n",
    "full_pipeline = ColumnTransformer([\n",
    "        # List of (name, transformer, columns) tuples specifying the\n",
    "        # transformer objects to be applied to subsets of the data.\n",
    "\n",
    "        (\"numericals_pipeline\", numericals_pipeline, numeric_attribs),\n",
    "        (\"categorical_pipeline\", OneHotEncoder(), cat_attribs),\n",
    "    ])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "\n",
    "num_samples, num_features = housing_prepared.shape\n",
    "print(f'''\n",
    "numbber of samples {num_samples}\n",
    "number of features {num_features}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models <a class=\"anchor\" id=\"train_models\"></a>\n",
    "\n",
    "here we try out 3 different estimators (models) and calculate the train error for each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "#######################\n",
    "#  Linear regression  #    \n",
    "#######################\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Define\n",
    "lin_reg = LinearRegression()\n",
    "# train \n",
    "lin_reg.fit(housing_prepared, housing_labels)\n",
    "# predict on training data \n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "# RMSE metric\n",
    "lin_rmse = mean_squared_error(housing_labels, housing_predictions, squared=False)\n",
    "print(f'Linear Regression train error {lin_rmse}')\n",
    "\n",
    "#######################\n",
    "#    Decision tree    #\n",
    "#######################\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Define\n",
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "# train\n",
    "tree_reg.fit(housing_prepared, housing_labels)\n",
    "# predict on training data \n",
    "housing_predictions = tree_reg.predict(housing_prepared)\n",
    "# RMSE metric\n",
    "tree_rmse = mean_squared_error(housing_labels, housing_predictions, squared=False)\n",
    "print(f'Regression Tree train error {tree_rmse}')\n",
    "\n",
    "###################\n",
    "#  Random Forest  #\n",
    "###################\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Define\n",
    "forest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# train\n",
    "forest_reg.fit(housing_prepared, housing_labels)\n",
    "# predict on training data \n",
    "housing_predictions = forest_reg.predict(housing_prepared)\n",
    "# RMSE metric\n",
    "forest_rmse = mean_squared_error(housing_labels, housing_predictions, squared=False)\n",
    "print(f'Random Forest train error {forest_rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation  <a class=\"anchor\" id=\"cross_validation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "\n",
    "for estimator in [lin_reg, tree_reg, forest_reg]:\n",
    "    # returns an array of scores of the estimator for each run of the cross validation.\n",
    "    scores = cross_val_score(estimator=estimator, X=housing_prepared, y=housing_labels,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "    model_type = estimator.__class__.__name__\n",
    "    rmse_scores = np.sqrt(-scores)\n",
    "    print(f'model type: {model_type}')\n",
    "    print('=========================')\n",
    "    display_scores(rmse_scores)\n",
    "    print('-------------------------')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest seems the most promising "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameter Optimization <a class=\"anchor\" id=\"hyperparam_optimization\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    # try 12 (3×4) combinations of hyperparameters\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    # then try 6 (2×3) combinations with bootstrap set as False\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "  ]\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \n",
    "# return train score\n",
    "# Computing training scores is used to get insights on how different\n",
    "# parameter settings impact the overfitting/underfitting trade-off.\n",
    "# However computing the scores on the training set can be computationally\n",
    "# expensive and is not strictly required to select the parameters that\n",
    "# yield the best generalization performance.\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=forest_reg, \n",
    "    param_grid=param_grid, \n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    return_train_score=True)\n",
    "\n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'''\n",
    "best params: {grid_search.best_params_}\n",
    "best estimator: {grid_search.best_estimator_}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we expect to see 18 rows - one for each HP combination\n",
    "grid_results = pd.DataFrame(grid_search.cv_results_)\n",
    "grid_results['rmse_score'] = np.sqrt(-grid_results.mean_test_score)\n",
    "grid_results.sort_values(by='rmse_score', ascending=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random hyper parameters search \n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_distribs = {\n",
    "        'n_estimators': randint(low=1, high=200), # defines a uniform discrete random variable to sample from \n",
    "        'max_features': randint(low=1, high=8),\n",
    "    }\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "rnd_search = RandomizedSearchCV(\n",
    "    estimator=forest_reg, \n",
    "    param_distributions=param_distribs,             # the parameter space to sample from (either rv or a list to sample from)\n",
    "    n_iter=10,                                      # Number of parameter settings that are sampled\n",
    "    cv=5,                                           # to specify the number of folds in a `(Stratified)KFold`\n",
    "    scoring='neg_mean_squared_error',               # the scoring function\n",
    "    random_state=42)\n",
    "\n",
    "rnd_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling two values from the discrete RV\n",
    "param_distribs['n_estimators'].rvs(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_results = pd.DataFrame(rnd_search.cv_results_)[['mean_test_score', 'params']]\n",
    "rnd_results['rmse_score'] = np.sqrt(-rnd_results.mean_test_score)\n",
    "rnd_results.sort_values(by='rmse_score', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best estimator \n",
    "rnd_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model pipeline <a class=\"anchor\" id=\"final_model\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "# using the feature extraction and the model explicitely \n",
    "# X_test_prepared = full_pipeline.transform(X_test)\n",
    "# final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "# setting them in a pipeline\n",
    "full_pipeline_with_predictor = Pipeline([\n",
    "        (\"preparation\", full_pipeline),\n",
    "        (\"rf_model\", final_model)\n",
    "])\n",
    "final_predictions = full_pipeline_with_predictor.predict(X_test)\n",
    "\n",
    "\n",
    "final_rmse = mean_squared_error(y_test, final_predictions, squared=False)\n",
    "print(f''' final RMSE {final_rmse} ''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "\n",
    "import joblib\n",
    "joblib.dump(value=full_pipeline_with_predictor, filename=\"my_model.pkl\") \n",
    "my_model_loaded = joblib.load(\"my_model.pkl\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_pipeline_with_predictor['preparation'].named_transformers_\n",
    "full_pipeline_with_predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features importance for Random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get categorical features (each category is a feature)\n",
    "cat_encoder = full_pipeline_with_predictor['preparation'].named_transformers_['categorical_pipeline']\n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[0]) # ['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN']\n",
    "\n",
    "extra_attribs = [\"rooms_per_household\", \"population_per_household\", \"bedrooms_per_room\"]\n",
    "\n",
    "# add to numeric and extra features\n",
    "attributes = numeric_attribs + extra_attribs + cat_one_hot_attribs\n",
    "\n",
    "rf_model = full_pipeline_with_predictor['rf_model']\n",
    "pd.DataFrame(zip(attributes, rf_model.feature_importances_), columns=['feature', 'importance']).sort_values(by='importance', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confidence interval for the RMSE score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "confidence = 0.95\n",
    "# take the square errors \n",
    "squared_errors = (final_predictions - y_test) ** 2\n",
    "# the average is t-distributed (unknown variance)\n",
    "np.sqrt(stats.t.interval(confidence, \n",
    "                         len(squared_errors) - 1, # degrees of freedom\n",
    "                         loc=squared_errors.mean(),\n",
    "                         scale=stats.sem(squared_errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or just assume normality :) which is ok for high enough dof\n",
    "\n",
    "zscore = stats.norm.ppf((1 + confidence) / 2)\n",
    "zmargin = zscore * squared_errors.std(ddof=1) / np.sqrt(m)\n",
    "np.sqrt(mean - zmargin), np.sqrt(mean + zmargin)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas profiler <a class=\"anchor\" id=\"pandas_profiler\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the installation take few minutes - \n",
    "# it is recommended running it on a clean notebook with only pandas imported and the dataset read.\n",
    "# when using matplotlib inline there could be some problems and errors\n",
    "\n",
    "# pip install -U pandas-profiling[notebook]\n",
    "# jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "profile = ProfileReport(df=housing, title=\"Housing Data Report\", explorative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}