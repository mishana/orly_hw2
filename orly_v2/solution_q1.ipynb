{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Assignment 1 - Q1 - Linear Regression with uncertainties in x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# inline plot\n",
    "%matplotlib inline\n",
    "# default figure size\n",
    "matplotlib.rcParams['figure.figsize'] = (20, 10)\n",
    "# to make our sets reproducible\n",
    "np.random.seed(42)\n",
    "%config Completer.use_jedi = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "first, read the .parquet file using pyarrow and then convert to a pandas DataFrame"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = Path('..') / 'data' / 'data1.parquet'  # assume there is a data folder in the parent path\n",
    "data = pq.read_table(data_path, memory_map=True)\n",
    "data = data.to_pandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "analyze the data by sampling and visually in a scatter plot. look for outliers."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.sample(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.plot.scatter('x', 'y', title='data scattter plot with outliers')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "we can get rid of the outliers by a simple cut-off on the x axis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_cutoff = data.loc[(data.y < -1500) & (data.y > -2000)].x.iloc[0]\n",
    "data = data.loc[data.x < x_cutoff]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "scatter plot the data without outliers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.plot.scatter('x', 'y', title='data scatter plot without outliers')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "now, try and fit a simple linear regressor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# include intercept\n",
    "lin_reg = LinearRegression(fit_intercept=True)\n",
    "lin_reg.fit(data.x.values.reshape(-1, 1), data.y)\n",
    "f'y = {lin_reg.intercept_} + {lin_reg.coef_[0]} * x'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.plot.scatter('x', 'y', title='data scatter plot without outliers and simple linear regressor', label='data')\n",
    "\n",
    "plt.plot(data.x, lin_reg.predict(data.x.values.reshape(-1, 1)), \"g-\", label='simple linear regressor')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "are the assumptions of linear regression correct? let's check it out."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "residuals = lin_reg.predict(data.x.values.reshape(-1, 1)) - data.y\n",
    "plt.scatter(x=lin_reg.predict(data.x.values.reshape(-1, 1)), y=residuals)\n",
    "plt.title('Residuals vs. Fitted')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "there's no pattern \"jumping to the eye\" in the 'Residuals vs. Fitted' plot, it seems uncorrelated. thus, probably the relationship is truly linear, and there's no sign of \"Heteroskedasticity\".\n",
    "also, it seems like there's no autocorrrelation in the residuals.\n",
    "third point is that there's only one independent variable x here, so no multicolinearity.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "perform a Shapiro-Wilks test to determine if the residuals are normally distributed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "\n",
    "shapiro(residuals)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "from the output we can see that the test statistic is 0.9937 and the corresponding p-value is 0.9275, which is a very high value and much higher than the usual cut-off p-value of 0.05. since the p-value is not less than .05, we fail to reject the null hypothesis. we do not have sufficient evidence to say that the sample data does not come from a normal distribution.\n",
    "meaning, the residuals seem normally distributed."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "we can happily confirm that a linear model is a good fit to the data. the only problem is that we ignored measurement errors in the independent variable x (which is an assumption in the simple linear regression model).\n",
    "to this end, this can be solved using Deming regression.\n",
    "we cite wiki here: \"In statistics, Deming regression, named after W. Edwards Deming, is an errors-in-variables model which tries to find the line of best fit for a two-dimensional dataset. It differs from the simple linear regression in that it accounts for errors in observations on both the x- and the y- axis. It is a special case of total least squares, which allows for any number of predictors and a more complicated error structure.\"\n",
    "seems like just the right tool for the job."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Deming regression assumes that the measurement errors of x and y are independent, and the ratio of their variances, between y error and x error, denoted by `delta`, is constant.\n",
    "in our case x-measurement is said to be twice as good as the y-measurement, in terms of error variance, meaning, $\\sigma_{error-x}^2 = \\frac{1}{2} \\cdot \\sigma_{error-y}^2$, and hence, `delta = 2`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cov = data.cov()\n",
    "mean_x = data.x.mean()\n",
    "mean_y = data.y.mean()\n",
    "s_xx = cov.x.x\n",
    "s_yy = cov.y.y\n",
    "s_xy = cov.x.y\n",
    "\n",
    "delta = 2\n",
    "slope = (s_yy  - delta * s_xx + np.sqrt((s_yy - delta * s_xx) ** 2 + 4 * delta * s_xy ** 2)) / (2 * s_xy)\n",
    "intercept = mean_y - slope  * mean_x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f'y = {intercept} + {slope} * x'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "now is a good time to visually analyze the difference in the two estimators and how they fit the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.plot.scatter('x', 'y')\n",
    "plt.plot(data.x, lin_reg.predict(data.x.values.reshape(-1, 1)), \"g-\", label='Linear Regression')\n",
    "plt.plot(data.x, [slope * x_val + intercept for x_val in data.x], \"r-\", label='Deming Regression')\n",
    "plt.legend()\n",
    "plt.title('Linear Regression vs. Deming Regression')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}