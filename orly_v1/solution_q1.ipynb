{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Homework 1 - Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# inline plot\n",
    "%matplotlib inline\n",
    "# default figure size\n",
    "matplotlib.rcParams['figure.figsize'] = (20, 10)\n",
    "# to make our sets reproducible\n",
    "np.random.seed(42)\n",
    "%config Completer.use_jedi = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "from typing import Tuple"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Read the Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_path = Path('..') / 'data' / 'data1.parquet'\n",
    "t = pq.read_table(data_path, memory_map=True)\n",
    "df = t.to_pandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### First Impressions - basic visual analysis of the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some basic statistics on the data:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can clearly spot, with a naked eye, three outlier points, and aside from that somewhat of a linear dependence of y on x:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.plot.scatter(x='x', y='y')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extract $X$ and $y$ into arrays:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = df.x.values.reshape(-1, 1)\n",
    "y = df.y.values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Linear Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, let us find out what a simple linear regression produces with the outliers:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# include intercept\n",
    "lin_reg = LinearRegression(fit_intercept=True)\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot the linear function - obtained from linear regression, along with the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(X, y, \"b.\")\n",
    "\n",
    "xs = np.linspace(int(X.min()), int(X.max()), 100)[:, np.newaxis]\n",
    "ys = lin_reg.predict(xs)\n",
    "plt.plot(xs, ys, \"r-\", label='fit')\n",
    "plt.title('Linear Regression model - data with outliers')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can easily get rid of the three outliers by filtering on $x \\geq 600$, for example:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "no_of_outliers = len(df.loc[df.x >= 600])\n",
    "assert no_of_outliers == 3, \"Outlier criteria is not good :(\"\n",
    "\n",
    "df_clean = df.loc[df.x < 600]\n",
    "df_clean.plot.scatter(x='x', y='y')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This looks more like a linear dependence. Let us look at some basic statistics on the \"cleaned\" data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_clean.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extract cleaned $X$ and $y$ arrays:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_clean = df_clean.x.values.reshape(-1, 1)\n",
    "y_clean = df_clean.y.values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let us model a linear regression on the \"cleaned\" data:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# include intercept\n",
    "lin_reg_clean = LinearRegression(fit_intercept=True)\n",
    "lin_reg_clean.fit(X_clean, y_clean)\n",
    "lin_reg_clean.intercept_, lin_reg_clean.coef_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(X_clean, y_clean, \"b.\")\n",
    "\n",
    "xs = np.linspace(int(X_clean.min()), int(X_clean.max()), 100)[:, np.newaxis]\n",
    "ys = lin_reg_clean.predict(xs)\n",
    "plt.plot(xs, ys, \"r-\")\n",
    "plt.title('Linear Regression model - cleaned data')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This looks good, although we didn't take into account measurement errors in $x$, which results in an underestimate of the coefficient, known as the attenuation bias.\n",
    "The current linear function is:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f'y = {lin_reg_clean.coef_[0]} * x + {lin_reg_clean.intercept_}'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Let us make sure that we are using the right model by validating linear regression assumptions:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 1. Linear and Additive:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_predicted = lin_reg_clean.predict(X_clean)\n",
    "\n",
    "residuals = y_predicted - y_clean\n",
    "plt.scatter(x=y_predicted, y=residuals)\n",
    "plt.title('Residuals vs. Fitted')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There is no pattern evident in the 'Residuals vs. Fitted' scatter plot. This suggests linearity in the data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 2. Autocorrelation:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from statsmodels.stats.stattools import durbin_watson\n",
    "durbin_watson(residuals)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see that the Durbin - Watson (DW) statistic is ~2. Which means there is no autocorrelation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 3. Multicollinearity: is irrelevant since we have only one independent variable."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 4. Heteroskedasticity:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looking back at the 'Residuals vs. Fitted' scatter plot, there is no funnel-shape pattern exhibited. Thus, there is no Heteroskedasticity."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 5. Normal Distribution of error terms:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us analyze the Normal Quantile-Quantile (Q-Q) plot:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy.stats import probplot\n",
    "\n",
    "_ = probplot(residuals, plot=plt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The plot shows a fairly straight line, which suggests a Normal distribution of the data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### All the linear regression assumptions checked out, so we continue assuming the dependence between $x$ and $y$ is best modeled with a linear function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "But now, let us try and get a more precise model, wich takes into account measurement errors in $x$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Deming Regression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We use [Deming regression](https://en.wikipedia.org/wiki/Deming_regression), which is a method that can be used when the errors for the two variables $x$ and $y$ are assumed to be independent and normally distributed, and the ratio of their variances, denoted δ, is known.\n",
    "In our case, it is given that \"You know that the technique used for measuring $x$ is\n",
    "twice as better than that for measuring $y$ in the sense of error variance, i.e. the error in $x$ is twice\n",
    "as small as the error in $y$\", so we conclude that δ = 2."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def deming_regression(data: pd.DataFrame) -> Tuple[float, float]:\n",
    "    delta = 2 # see discussion above\n",
    "\n",
    "    cov = data.cov()\n",
    "    mean_x = data['x'].mean()\n",
    "    mean_y = data['y'].mean()\n",
    "    s_xx = cov['x']['x']\n",
    "    s_yy = cov['y']['y']\n",
    "    s_xy = cov['x']['y']\n",
    "\n",
    "    coeff = (s_yy  - delta * s_xx + np.sqrt((s_yy - delta * s_xx) ** 2 + 4 * delta * s_xy ** 2)) / (2 * s_xy)\n",
    "    intercept = mean_y - coeff  * mean_x\n",
    "\n",
    "    return coeff, intercept"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "coeff, intercept = deming_regression(df_clean)\n",
    "coeff, intercept"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using Deming Regression, we get different values for the coefficient and the intercept, compared with the Linear Regression one.\n",
    "The new model is:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f'y = {coeff} * x + {intercept}'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us compare the two linear estimators on the same plot:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.scatter(x=X_clean, y=y_clean)\n",
    "plt.plot(X_clean, lin_reg_clean.predict(X_clean), label='Ignoring errors in $x$')\n",
    "plt.plot(X_clean, [intercept + coeff * x for x in X_clean], label='Accounting for errors in $x$')\n",
    "plt.legend()\n",
    "plt.title('Linear Regression vs. Deming Regression')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}